{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "from natsort import natsorted\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import OrderedDict\n",
    "from scipy.ndimage import zoom\n",
    "from glob import glob\n",
    "import nibabel as nib\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline for dataloader\n",
    "- reads in the original nifti 3d image\n",
    "- reads in the gradCAM\n",
    "- based on the gradCAM, choose the most significant slice of the nifti image\n",
    "    - choose 3 different slices from the 3 directions   \n",
    "- output the 3 slices in 3 channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsize_transform(data): \n",
    "    target_size = (192, 192, 192)\n",
    "    data = torch.from_numpy(data).unsqueeze(0).unsqueeze(0)\n",
    "    downsampled = torch.nn.functional.interpolate(data, size=target_size, mode='trilinear')\n",
    "\n",
    "    return downsampled.squeeze(0).squeeze(0)\n",
    "\n",
    "class TransformerDataset(Dataset):\n",
    "    def __init__(self, img_dir, grad_dir, transforms = None):\n",
    "        self.img_dir = img_dir\n",
    "        self.grad_dir = grad_dir\n",
    "        self.transforms = transforms\n",
    "        self.cn_dir = os.path.join(self.img_dir, \"MNI152_affine_WB_iso1mm/CN\")\n",
    "        self.scz_dir = os.path.join(self.img_dir, \"MNI152_affine_WB_iso1mm/schiz\")\n",
    "        self.grad_cn_dir = os.path.join(self.grad_dir, \"MNI152_affine_WB_iso1mm/CN\")\n",
    "        self.grad_scz_dir = os.path.join(self.grad_dir, \"MNI152_affine_WB_iso1mm/CN\") # change this later when rerun the extract grad script\n",
    "        self.samples, self.labels = self._load_samples()\n",
    "\n",
    "    def _load_samples(self):\n",
    "        samples = []\n",
    "        \n",
    "        samples = [file for file in os.listdir(self.cn_dir) if file.endswith(\".nii.gz\")]\n",
    "        labels = [0] * len(samples)\n",
    "        samples += [file for file in os.listdir(self.scz_dir) if file.endswith(\".nii.gz\")]\n",
    "        labels += [1] * (len(samples) - len(labels))\n",
    "\n",
    "        return samples, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        k = 3\n",
    "        label = self.labels[idx]\n",
    "        grad_file = self.samples[idx].split(\".\")[0]\n",
    "        if label == 0:\n",
    "            file_path = os.path.join(self.cn_dir, self.samples[idx])\n",
    "            grad_path = os.path.join(self.grad_cn_dir, grad_file+ \".nii_activation.nii.gz\")\n",
    "        else:\n",
    "            file_path = os.path.join(self.scz_dir, self.samples[idx])\n",
    "            grad_path = os.path.join(self.grad_scz_dir, grad_file+ \".nii_activation.nii.gz\")\n",
    "        one_hot_label = torch.zeros(2)\n",
    "        one_hot_label[label] = 1\n",
    "        label = one_hot_label\n",
    "\n",
    "        # Load the NIfTI image\n",
    "        img = nib.load(file_path)\n",
    "\n",
    "        \n",
    "        grad = nib.load(grad_path)\n",
    "\n",
    "        # Get the image data array\n",
    "        img_data = np.float32(img.get_fdata())\n",
    "        img_data = self.transforms(img_data)\n",
    "\n",
    "        grad_data = np.float32(grad.get_fdata())\n",
    "        sums_x = np.sum(grad_data, axis=(1, 2))\n",
    "        sums_y = np.sum(grad_data, axis=(0, 2))\n",
    "        sums_z = np.sum(grad_data, axis=(0, 1))\n",
    "\n",
    "        # Find the indices of the maximum sum along each axis\n",
    "        # max_x_index = np.argmax(sums_x)\n",
    "        x_slices = np.argsort(sums_x)[::-1][:k]\n",
    "        x_copy = x_slices.copy()\n",
    "        y_slices = np.argsort(sums_y)[::-1][:k]\n",
    "        y_copy = y_slices.copy()\n",
    "        z_slices = np.argsort(sums_z)[::-1][:k]\n",
    "        z_copy = z_slices.copy()\n",
    "\n",
    "        # max_y_index = np.argmax(sums_y)\n",
    "        # max_z_index = np.argmax(sums_z)\n",
    "\n",
    "        x_slice = img_data[x_copy, :, :]\n",
    "        y_slice = img_data[:, y_copy, :].reshape((3, 192, 192))\n",
    "        z_slice = img_data[:, :, z_copy].reshape((3, 192, 192))\n",
    "\n",
    "        return np.concatenate((x_slice, y_slice, z_slice), axis = 0), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.91557547, 0.09639896],\n",
       "        [0.9508808 , 0.07190435],\n",
       "        [0.79231897, 0.15145252]],\n",
       "\n",
       "       [[0.0461586 , 0.53150262],\n",
       "        [0.75110032, 0.81654726],\n",
       "        [0.85463303, 0.65851167]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.rand(3, 2)\n",
    "\n",
    "y = np.random.rand(3, 2)\n",
    "np.array([x, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/media/youzhi/SSD/bme_project/activations/fold1',\n",
       " '/media/youzhi/SSD/bme_project/activations/fold2',\n",
       " '/media/youzhi/SSD/bme_project/activations/fold3',\n",
       " '/media/youzhi/SSD/bme_project/activations/fold4',\n",
       " '/media/youzhi/SSD/bme_project/activations/fold5',\n",
       " '/media/youzhi/SSD/bme_project/activations/fold6',\n",
       " '/media/youzhi/SSD/bme_project/activations/fold7',\n",
       " '/media/youzhi/SSD/bme_project/activations/fold8',\n",
       " '/media/youzhi/SSD/bme_project/activations/fold9',\n",
       " '/media/youzhi/SSD/bme_project/activations/fold10']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = \"/media/youzhi/SSD/bme_project/data\"\n",
    "grad_root_dir = \"/media/youzhi/SSD/bme_project/activations\"\n",
    "folds_dir = [dir for dir in os.listdir(root_dir) if dir.startswith(\"fold\")]\n",
    "grads_dir = [os.path.join(grad_root_dir, dir) for dir in folds_dir]\n",
    "folds_dir = [os.path.join(root_dir, dir) for dir in folds_dir]\n",
    "folds_dir = natsorted(folds_dir)\n",
    "grads_dir = natsorted(grads_dir)\n",
    "grads_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1 :  196\n",
      "fold 2 :  188\n",
      "fold 3 :  187\n",
      "fold 4 :  185\n",
      "fold 5 :  187\n",
      "fold 6 :  185\n",
      "fold 7 :  187\n",
      "fold 8 :  182\n",
      "fold 9 :  188\n",
      "fold 10 :  187\n"
     ]
    }
   ],
   "source": [
    "dataloaders = []\n",
    "for i in range(len(folds_dir)):\n",
    "    fold_dir = folds_dir[i]\n",
    "    grad_dir = grads_dir[i]\n",
    "    dataset = TransformerDataset(fold_dir, grad_dir, downsize_transform) #, downsize_transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "    dataloaders.append(dataloader)\n",
    "    print(\"fold\", i+1, \": \", len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 9, 192, 192])\n",
      "<built-in method type of Tensor object at 0x75fe2ed656d0>\n",
      "tensor([[1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.]])\n",
      "torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(dataloaders[0]))\n",
    "# plt.imshow(sample[0][0][0][:, int(96/2), :], cmap = 'bone')\n",
    "print(sample[0].shape)\n",
    "print(sample[0].type)\n",
    "print(sample[1])\n",
    "print(sample[1].shape)\n",
    "# plt.imshow(sample[0][0][2], cmap = 'bone')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmeProj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
